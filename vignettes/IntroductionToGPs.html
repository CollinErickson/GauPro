<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Collin Erickson" />

<meta name="date" content="2017-08-16" />

<title>Introduction to Gaussian Processes</title>






<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Introduction to Gaussian Processes</h1>
<h4 class="author"><em>Collin Erickson</em></h4>
<h4 class="date"><em>2017-08-16</em></h4>



<p>A Gaussian process is a stochastic process that assumes that the outputs for any set of input points follows a multivariate normal distribution. To determine the normal distribution, we must select a mean function that gives a mean for each point and a covariance function that gives the covariance between any set of points.</p>
<p>Thus if we have mean function <span class="math inline">\(\mu\)</span>, covariance function <span class="math inline">\(C\)</span>, and the <span class="math inline">\(n \times d\)</span> matrix X with the input vectors <span class="math inline">\(\mathbf{x_1}, ..., \mathbf{x_n}\)</span> in its rows, then distribution of the output at these points, <span class="math inline">\(\mathbf{y} = [y_1, \ldots, y_n]^T\)</span> is given by:</p>
<p><span class="math display">\[ \mathbf{y} \sim  N(\mu(X),~C(X)) \]</span></p>
<p>Or in full element notation:</p>
<p><span class="math display">\[ \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} \sim N(\begin{bmatrix} \mu(\mathbf{x_1}) \\ \vdots \\ \mu(\mathbf{x_n}) \end{bmatrix}, \begin{bmatrix} C(\mathbf{x_1},\mathbf{x_1}) &amp;\cdots &amp;C(\mathbf{x_1},\mathbf{x_n}) \\ \vdots &amp;\ddots &amp;\vdots \\ C(\mathbf{x_n},\mathbf{x_1}) &amp;\cdots &amp;C(\mathbf{x_n},\mathbf{x_n}) \end{bmatrix})\]</span></p>
<div id="mean-function-mu" class="section level2">
<h2>Mean function <span class="math inline">\(\mu\)</span></h2>
<p>The mean function can be any function mapping the input space to the real numbers. The most commonly used mean function is a constant, so <span class="math inline">\(\mu(\mathbf{x}) = \mu\)</span>. This means that over the entire space the predicted mean given no other information a constant. When fitting a GP model to data, <span class="math inline">\(\mu\)</span> is usually estimated using the data. Another commonly used mean function is zero. This works surprisingly well since the GP will interpolated between your data, meaning that the mean function work have much of an effect when there is enough data.</p>
<p>A more advanced choice of mean function is to use a linear model, so <span class="math display">\[\mu(\mathbf{x}) = \beta_0 + \sum_{i=1}^d \beta_i x_i\]</span>. Again these parameters must be estimated. This can be generalized to a linear combination of functions of the input data, <span class="math inline">\(f_1, \ldots, f_m\)</span> so the mean function is <span class="math display">\[\mu(\mathbf{x}) = \beta_0 + \sum_{i=1}^m \beta_i f_i(\mathbf{x})\]</span>.</p>
<p>It is generally recommended to just use a constant mean since the data itself should provide enough information to fit the true function. Some researchers say that using a linear model can have negative effects on fitting a good model.</p>
</div>
<div id="covariance-function-c" class="section level2">
<h2>Covariance function <span class="math inline">\(C\)</span></h2>
<p>The covariance function determines the “rough” the surface is, or how strong the correlation is between points. Notation: A covariance/correlation function should take two vectors as input. For a <span class="math inline">\(n \times d\)</span> matrix <span class="math inline">\(X\)</span>, <span class="math inline">\(C(X)\)</span> denotes the <span class="math inline">\(n \times n\)</span> matrix whose <span class="math inline">\((i, j)\)</span> element is <span class="math inline">\(C(\mathbf{x_i}, \mathbf{x_j})\)</span>.</p>
<p>Often a correlation function, <span class="math inline">\(R\)</span>, is used instead of a covariance function. The correlation function should map any pair of points to <span class="math inline">\([0,1]\)</span>. The correlation for any point with itself should be 1, i.e. <span class="math inline">\(R(\mathbf{x}, \mathbf{x}) = 1\)</span>. When a correlation function is used, a variance parameter <span class="math inline">\(\sigma^2\)</span> must be estimated to scale the correlation matrix into a covariance matrix. Thus the covariance matrix is <span class="math inline">\(C(X) = \hat{\sigma}^2 R(X)\)</span></p>
<div id="gaussian-correlation" class="section level3">
<h3>Gaussian correlation</h3>
<p>The most commonly used correlation function is the Gausian.</p>
<p><span class="math display">\[ R(\mathbf{u}, \mathbf{v}) = \exp{\sum_{i=1}^d} \theta_i (u_i - v_i)^2\]</span></p>
<p>The parameters <span class="math inline">\(\mathbf{\theta} = (\theta_1, \ldots, \theta_d)\)</span> are the correlation parameters for each dimensions. Generally they must be estimated from the data when fitting a Gaussian process model to data.</p>
</div>
</div>
<div id="likelihood-function-and-parameter-estimation" class="section level2">
<h2>Likelihood function and parameter estimation</h2>
<p>The parameters are often estimated by finding the parameters that maximize the likelihood given a data set.</p>
<p>The likelihood function is the usual multivariate normal pdf shown below, where <span class="math inline">\(\mathbf{\mu} = \mu(X)\)</span>, <span class="math inline">\(\Sigma = C(X)\)</span></p>
<p><span class="math display">\[ f(\mathbf{\theta};~X,~\mathbf{y}) = f(X,~\mathbf{y};~\mathbf{\theta}) = \frac{1}{(2 \pi)^{n/2} |\Sigma|^{1/2} } \exp{(-\frac{1}{2} (\mathbf{y} - \mathbf{\mu})^T \Sigma^{-1} (\mathbf{y} - \mathbf{\mu}))}\]</span></p>
<p>As usual, we use negative two times the log-likelihood for simplicity, ignoring the constant terms.</p>
<p><span class="math display">\[ \ell(\theta) = \ln |\Sigma| + (\mathbf{y} - \mathbf{\mu})^T \Sigma^{-1} (\mathbf{y} - \mathbf{\mu})\]</span></p>
<p>This equation is minimized as a function of the correlation parameters to find the parameters that give the greatest likelihood. Since there is a determinant and matrix solve, this is an expensive function to optimize, with each evaluation being <span class="math inline">\(O(n^3)\)</span></p>
<div id="estimate-for-constant-mu" class="section level3">
<h3>Estimate for constant <span class="math inline">\(\mu\)</span></h3>
<p>If the mean is set to be constant, <span class="math inline">\(\mu(X) = \mu \mathbf{1_n}\)</span>, then there is a single parameter <span class="math inline">\(\mu\)</span> to estimate. Differentiating <span class="math inline">\(\ell\)</span> with respect to <span class="math inline">\(\mu\)</span> will then give <span class="math display">\[ \frac{d \ell}{d \mu} = \mathbf{1_n}^T \Sigma^{-1}(\mathbf{y} - \mu \mathbf{1_n})\]</span> Setting this equal to zero and solving for <span class="math inline">\(\mu\)</span> gives the maximum likelihood estimate <span class="math inline">\(\hat{\mu}\)</span> <span class="math display">\[ \hat{\mu} = \frac{\mathbf{1_n}^T \Sigma^{-1}\mathbf{y}}{\mathbf{1_n}^T \Sigma^{-1}\mathbf{1_n}}\]</span></p>
</div>
<div id="estimate-for-sigma" class="section level3">
<h3>Estimate for <span class="math inline">\(\sigma\)</span></h3>
<p>When using a correlation matrix so that <span class="math inline">\(\Sigma = \sigma^2 R\)</span>, <span class="math inline">\(\sigma\)</span> must be estimated using maximum likelihood.</p>
<p><span class="math display">\[ \frac{d \ell}{d \sigma^2} = \frac{n}{\sigma^2} - \frac{1}{\sigma^4}(\mathbf{y} - \mathbf{\mu})^T R^{-1} (\mathbf{y} - \mathbf{\mu})\]</span> Setting equal to zero and solving for <span class="math inline">\(\sigma^2\)</span> gives <span class="math display">\[ \hat{\sigma}^2 = \frac{1}{n} (\mathbf{y} - \mathbf{\mu})^T R^{-1} (\mathbf{y} - \mathbf{\mu})\]</span> When estimating <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> simultaneously, these estimates are valid and the estimate can simply be plugged into the equation.</p>
</div>
</div>
<div id="prediction-of-new-points" class="section level2">
<h2>Prediction of new points</h2>
<div id="conditional-distribution" class="section level3">
<h3>Conditional distribution</h3>
<p>Suppose there are vectors <span class="math inline">\(\mathbf{y_1}\)</span> and <span class="math inline">\(\mathbf{y_2}\)</span> that are jointly multivariate normal. The joint distribution is <span class="math display">\[ \begin{bmatrix} \mathbf{y_1} \\ \mathbf{y_2} \end{bmatrix} \sim N(
\begin{bmatrix} \mathbf{\mu_1} \\ \mathbf{\mu_2} \end{bmatrix},
~\begin{bmatrix} \Sigma_{11} \Sigma_{12} \\ \Sigma_{21} \Sigma_{22} \end{bmatrix} )
\]</span></p>
<p>The conditional distribution of <span class="math inline">\(\mathbf{y_1}\)</span> given <span class="math inline">\(\mathbf{y_2}\)</span> is</p>
<p><span class="math display">\[\mathbf{y_1} ~|~\mathbf{y_2} \sim 
N(\mathbf{\mu_1} + \Sigma_{12} \Sigma_{22}^{-1}( \mathbf{y_2} - \mathbf{\mu_2})),
~\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})
\]</span></p>
</div>
<div id="predicting" class="section level3">
<h3>Predicting</h3>
<p>Suppose there are two input matrices, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, whose rows are the input points, with corresponding output vectors <span class="math inline">\(\mathbf{y_1}\)</span> and <span class="math inline">\(\mathbf{y_2}\)</span>. Suppose we have the actual values for <span class="math inline">\(\mathbf{y_2}\)</span>, and want to estimate, or predict, <span class="math inline">\(\mathbf{y_1}\)</span>. We can use the conditional distribution above to get a posterior distribution for <span class="math inline">\(\mathbf{y_1}\)</span>.</p>
<p>If we only want to predict for a single point, i.e. we want to predict the output <span class="math inline">\(y\)</span> at <span class="math inline">\(\mathbf{x}\)</span>, then this equation gives</p>
<p><span class="math display">\[y ~|~\mathbf{y_2} \sim 
N(\hat{y},
~\hat{\sigma}^2(y))
\]</span> where <span class="math display">\[ \hat{y} = \hat{\mu} + R(\mathbf{x},~X_2) R(X_2)^{-1}( \mathbf{y_2} - \mu\mathbf{1_n})) \]</span> and <span class="math display">\[ \hat{\sigma}^2(y) = R(\mathbf{x}) - R(\mathbf{x},~X_2) R(X_2)^{-1} R(X_2,~\mathbf{x}) \]</span></p>
<p>Notice we get an estimate not only for the value of <span class="math inline">\(y\)</span>, but also the standard error. This can be useful when we need a way to judge the prediction accuracy of the model.</p>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>

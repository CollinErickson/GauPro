<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Collin Erickson" />

<meta name="date" content="2022-11-09" />

<title>Derivatives for estimating Gaussian process parameters</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Derivatives for estimating Gaussian process
parameters</h1>
<h4 class="author">Collin Erickson</h4>
<h4 class="date">2022-11-09</h4>



<p>Estimating the parameters is often done through maximum likelihood
estimation. In this vignette, derivatives of parameters are calculated
to give the parameter gradient. This can then be used in numerical
optimization, such as with BFGS, and should be much faster than
calculating without having the gradient.</p>
<div id="deviance" class="section level2">
<h2>Deviance</h2>
<p>In this vignette, I follow the lead of the <a href="https://doi.org/10.18637/jss.v064.i12">GPfit paper</a>. Instead of
maximizing the likelihood, we find parameter estimates by minimizing the
deviance. For correlation parameters <span class="math inline">\(\theta\)</span>, the deviance is shown below.</p>
<p><span class="math display">\[ -2\log L_{\theta} \propto \log |R| +
n\log[(Y-1_n \hat{\mu}(\theta))^T R^{-1} (Y-1_n \hat{\mu}(\theta))]  =
\mathcal{D}\]</span></p>
<p>For now I will assume that <span class="math inline">\(\hat{\mu}\)</span> does not depend on <span class="math inline">\(\theta\)</span>, and will replace <span class="math inline">\(Y-\hat{\mu}(\theta)\)</span> with <span class="math inline">\(Z\)</span>, as shown below.</p>
<p><span class="math display">\[ \mathcal{D} = \log |R| + n\log[Z^T
R^{-1} Z]\]</span></p>
<p>Thus the only dependence on the correlation parameters is through
<span class="math inline">\(R\)</span>.</p>
<p>Via the <a href="https://gaussianprocess.org/gpml/chapters/RWA.pdf">GPML book
section A.3.1</a>, we need the following equations:</p>
<p><span class="math display">\[  \frac{\partial }{\partial \theta}
R^{-1} = -R^{-1} \frac{\partial R}{\partial \theta} R^{-1}\]</span></p>
<p><span class="math display">\[  \frac{\partial }{\partial \theta} \log
|R|  = \text{tr}(R ^ {-1}\frac{\partial R}{\partial \theta}
)   \]</span></p>
<p>Now we can calculate the derivative of the deviance in terms of <span class="math inline">\(\frac{\partial R}{\partial \theta}\)</span>.</p>
<p><span class="math display">\[ \frac{\partial \mathcal{D}}{\partial
\theta} = \text{tr}(R ^ {-1}\frac{\partial R}{\partial \theta} ) -  
\frac{n}{Z^T R^{-1} Z} Z^T R^{-1} \frac{\partial R}{\partial \theta}
R^{-1} Z
\]</span></p>
<p>Now we just need to find <span class="math inline">\(\frac{\partial
R}{\partial \theta}\)</span>, which depends on the specific correlation
function <span class="math inline">\(R(\theta)\)</span>.</p>
</div>
<div id="nugget" class="section level2">
<h2>Nugget</h2>
<p>The correlation function is usually augmented by adding a nugget term
<span class="math inline">\(\delta\)</span> to the diagonal:</p>
<p><span class="math display">\[ R = R^* + \delta I  \]</span></p>
<p>The nugget accounts for noise in the responses, smoothing the
predicted response function. It also helps with numerical stability,
which is a serious problem when there is a lot of data. Often a small
value is used even the function is noiseless.</p>
<p>For the nugget,</p>
<p><span class="math display">\[\frac{\partial R}{\partial \delta} =
I\]</span></p>
<p>Thus, the derivative for the deviance is very simple. <span class="math display">\[ \frac{\partial \mathcal{D}}{\partial \delta} =
\text{tr}(R ^ {-1} ) -  
\frac{n}{Z^T R^{-1} Z} Z^T R^{-1}  R^{-1} Z
\]</span></p>
<p>This equation gives the derivative of the deviance with respect to
the nugget regardless of the correlation function used.</p>
</div>
<div id="gaussian-correlation" class="section level2">
<h2>Gaussian correlation</h2>
<p>The Gaussian correlation function has parameter vector <span class="math inline">\(\theta = (\theta_1, ..., \theta_d)\)</span>.</p>
<p>Ignoring the nugget term, the <span class="math inline">\(i\)</span>,<span class="math inline">\(j\)</span>
entry of the correlation matrix is</p>
<p><span class="math display">\[  R_{ij}(x, y) =
\exp\bigg[-\sum_{k=1}^{d} \theta_k (x_{ik} - x_{jk})^2
\bigg]  \]</span></p>
<p><span class="math display">\[  \frac{\partial}{\partial \theta_l}
R_{ij}(x, y) = -\exp\bigg[-\sum_{k=1}^{d} \theta_k (x_{ik} - x_{jk})^2
\bigg] (x_{il} - x_{jl})^2  \]</span></p>
<p>This will give the matrix <span class="math inline">\(\frac{\partial
R}{\partial \theta_l}\)</span>, which can be used with the previous
equations to calculate <span class="math inline">\(\frac{\partial
\mathcal{D}}{\partial \theta_l}\)</span>.</p>
</div>
<div id="lifted-brownian-covariance" class="section level2">
<h2>Lifted brownian covariance</h2>
<p>The <a href="https://amstat.tandfonline.com/doi/abs/10.1080/00401706.2016.1211555">lifted
brownian covariance</a> function is <span class="math display">\[  c(x,
x&#39;) = \psi(x) + \psi(x&#39;) + \psi(x-x&#39;) - 1  \]</span> where
<span class="math display">\[ \psi(h) = (1 + ||h||_a^{2\gamma} )
^{\beta} \]</span></p>
<p>and <span class="math display">\[ ||h||_a^2 = \boldsymbol{h}^T
\begin{bmatrix}
    a_1 &amp; 0 &amp; \dots &amp; 0 \\
    0 &amp; a_2 &amp;  \dots  &amp; 0 \\
    \vdots &amp; \vdots &amp;  \ddots &amp; \vdots \\
    0 &amp; 0  &amp; \dots  &amp; a_d
\end{bmatrix}
\boldsymbol{h} = \sum_{i=1}^d a_i h_i^2\]</span></p>
<p>This is different from the others because it is not a correlation
function, which ranges from 0 to 1, but is the covariance function
itself. Thus we will have to use first deviance before inserting <span class="math inline">\(\hat{\sigma}^2\)</span>.</p>
<p><span class="math display">\[ \frac{\partial}{\partial \beta} \psi(h)
=   (1 + ||h||_a^{2\gamma} ) ^{\beta} \log (1 + ||h||_a^{2\gamma} ) =
\psi(h) \log (1 + ||h||_a^{2\gamma} ) \]</span></p>
<p><span class="math display">\[ \frac{\partial}{\partial \gamma}
\psi(h) =  \beta (1 + ||h||_a^{2\gamma} ) ^{\beta-1} ||h||_a^{2\gamma}
\log(||h||_a^{2})  \]</span> <span class="math display">\[
\frac{\partial}{\partial a_i} ||h||_a^{2} = h_i^2  \]</span></p>
<p><span class="math display">\[ \frac{\partial}{\partial a_i}
||h||_a^{2\gamma} = \gamma ||h||_a^{2(\gamma-1)} h_i^2 \]</span></p>
<p><span class="math display">\[ \frac{\partial}{\partial a_i} \psi(h)
=  \beta (1 + ||h||_a^{2\gamma} ) ^{\beta-1} \gamma
||h||_a^{2(\gamma-1)} h_i^2  \]</span></p>
</div>
<div id="likelihood" class="section level2">
<h2>Likelihood</h2>
<p>The likelihood for data from a Gaussian process follows the standard
multivariate normal probability distribution function (pdf). <span class="math display">\[  L = (2 \pi)^{-k/2} |\Sigma|^{-1/2}
\exp[\frac{-1}{2}(Y - \mu) \Sigma^{-1} (Y - \mu)]  \]</span> The log
likelihood is generally easier to work with.</p>
<p><span class="math display">\[  \log L = \frac{-k}{2} \log(2 \pi)
+  \frac{-1}{2}\log|\Sigma|  +  \frac{-1}{2}(Y - \mu) \Sigma^{-1} (Y -
\mu)  \]</span> To simplify, we can multiply it by -2, and call this the
deviance, denoted here as <span class="math inline">\(\mathcal{D}\)</span>.</p>
<p><span class="math display">\[ \mathcal{D} = -2\log L = k \log(2 \pi)
+  \log|\Sigma|  +  (Y - \mu) \Sigma^{-1} (Y - \mu)  \]</span> <span class="math inline">\(k\)</span> can be ignored since it usually
constant while optimizing parameters. <span class="math display">\[
\mathcal{D} = -2\log L \propto  \log|\Sigma|  +  (Y - \mu) \Sigma^{-1}
(Y - \mu)  \]</span></p>
<p><span class="math display">\[ \frac{\partial \mathcal{D}}{\partial
\theta} = \text{tr}(\Sigma ^ {-1}\frac{\partial \Sigma}{\partial \theta}
) -  
Z^T \Sigma^{-1} \frac{\partial \Sigma}{\partial \theta} \Sigma^{-1} Z
\]</span></p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
